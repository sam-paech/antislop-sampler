{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Dict, Tuple, Generator, Set\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from ipywidgets import Output\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "word_penalties_list = [['a kaleidoscope of', 2000], ['testament', 2000.0150485530914], ['technopolis', 1682.682059035117], ['understandingly', 762.9294022671543], ['paperbound', 659.2264970913199], ['hesitantly', 496.5646879026894], ['piqued', 482.3001178804444], ['delved', 473.4940223966827], ['curveballs', 462.50687039417824], ['bustling', 454.70303449492854], ['marveled', 428.19439049963717], ['inclusivity', 399.28185144068397], ['birdwatcher', 382.93952575702605], ['elara', 382.02399833524635], ['camaraderie', 325.065910926091], ['newfound', 289.3537643476301], ['marveling', 281.4117889244332], [\"hiroshi's\", 277.20734354116485], ['greentech', 268.92005660614404], ['thoughtfully', 266.9326102346037], ['intently', 251.51633784078055], ['birdwatching', 250.1588231011304], ['amidst', 249.22122673588677], ['cherishing', 247.91009553317267], ['attentively', 246.79285812826976], ['interjected', 235.9610251843368], ['serendipitous', 233.0266906486917], [\"marianne's\", 232.87022859034334], [\"maya's\", 230.37564440034032], ['excitedly', 228.84649139211055], ['steepled', 228.70847810531137], ['engrossed', 228.4472196666415], ['fostering', 222.59645412759878], ['brainstormed', 218.63487421031408], ['furrowed', 217.1288191216257], ['nodded', 215.746532494257], ['contemplatively', 213.9394730581084], ['jotted', 212.19052857841066], [\"mia's\", 209.54359039341304], ['yesteryears', 205.40375361782048], ['conspiratorially', 204.18903883197223], ['poring', 203.12158920489887], ['stumbled', 201.95286430826962], ['strategized', 198.31538808865406], ['hesitated', 194.35575102206053], ['intrigued', 191.32480777377384], [\"sarah's\", 188.1056806342427], ['lykos', 186.8984432180823], ['adaptability', 185.2743410645729], ['yoing', 184.27349339389698], ['geocaches', 182.61995131301913], ['furrowing', 181.28300434012144], ['quandaries', 178.45513005800902], ['chimed', 177.6317240627814], ['headfirst', 177.55430176035384], ['gruffly', 173.5169670342752], ['skeptically', 173.2196196510284], ['nestled', 170.24886793134038], ['fruitville', 168.93895251122095], ['gastronomical', 168.77834340202367], ['sighed', 167.83599102428747], ['warmly', 166.64524795750117], ['approvingly', 165.14554435242388], ['questioningly', 164.2217764827755], [\"timmy's\", 162.85237720972512], ['undeterred', 159.81034467083455], ['starlit', 158.81973280586473], ['unearthing', 157.53953848282245], ['grappled', 155.11380760257956], ['couldn', 154.07882616965472], [\"yumi's\", 153.362396079487], [\"seabrook's\", 152.65396517679832], ['geocachers', 152.48899331241418], ['animatedly', 150.64516344395025], ['bakersville', 149.48324667712868], ['minji', 148.7787149817242], ['fateful', 147.881376001738], ['sparkled', 145.48284973440963], ['resonated', 144.91492949803347], ['harmoniously', 144.8378436549682], ['fidgeted', 143.88462648395776], ['mwanga', 141.271194443305], ['gleamed', 140.84454272803274], ['embracing', 140.8134127640521], ['pleasantries', 138.9683910665212], ['iostream', 137.02499195670802], ['navigated', 136.8749045617025], ['interconnectedness', 136.6775722710472], ['tanginess', 136.0248012468762], ['mouthwatering', 135.40207079890078], [\"amelia's\", 135.12735430462644], ['delving', 134.62133115310917], ['mischievously', 134.53400914082113], ['tirelessly', 134.50459651470058], ['transcended', 132.75875026522667], ['sympathetically', 132.28731274201124], ['pondered', 132.24181930810568], ['lingered', 131.6820547398057], ['println', 131.53501960158073], ['empathizing', 130.38734974729505], ['niche', 128.82354262722254], ['regaled', 128.21211629309659], ['greenthumb', 127.87603715586023], ['savored', 127.44044593637169], [\"amira's\", 127.26977675143385], ['meticulously', 125.67264078678225], ['firsthand', 123.40718461639742], ['empathetically', 122.76583436768932], ['unshed', 122.234447281337], [\"jenkin's\", 122.13793091468249], ['empathy', 120.78510640297107], ['enigmatically', 120.10278606401909], [\"marla's\", 119.96311948139385], ['bayville', 119.86205591147561], ['adversities', 119.16540991510242], ['eagerly', 118.7736944560049], ['labyrinthine', 117.30247757246565], ['quizzically', 116.99368259297609], ['transcending', 116.98548707285242], ['resilience', 115.03290831887757], [\"lily's\", 114.79275367950875], ['commiserated', 114.67810631179985], ['savoring', 114.09168105940152], [\"amara's\", 113.35572623503091], ['somberly', 111.33701848917218], ['cinephile', 110.08462885614495], ['solace', 109.58942409221955], ['twinkled', 108.8491511689895], ['aquascaping', 108.490673606918], ['rippled', 107.44694800406151], ['reveled', 107.0468756808211], ['greenhaven', 106.94331659047654], ['birdwatchers', 106.78153731714639], ['adwoa', 105.93106020380063], ['appreciatively', 105.79259888560438], ['awestruck', 105.76861040590829], ['ecotech', 105.3920442928442], ['navigating', 105.00722107822565], ['wasn', 104.19334186895765], ['lightheartedness', 103.97167324805294], ['disapprovingly', 103.7632507571004], ['exclaimed', 103.50051962551075], [\"samir's\", 103.32935174824694], ['fishkeeping', 102.85891331779236], ['sparked', 101.88166406410981], ['welled', 101.1238612235024], ['jotting', 101.00457387540926], ['resourcefulness', 100.7218012566337], ['flickered', 99.7346200354375], ['reminisced', 99.40737436442414], [\"abernathy's\", 97.97170234043274], ['unbeknownst', 96.90280381444087], ['pattered', 96.45923446705675], ['reassuringly', 95.99564132547866], ['miscommunications', 95.92537976660182], ['wafted', 95.46404472730974], ['absentmindedly', 94.91609082523561], ['weightiness', 94.72530273625273], ['allyship', 94.20061224332532], ['perseverance', 93.51317590165482], ['timmy', 93.37738761768523], ['mindfully', 92.98616009115628], ['hadn', 92.85616357020969], ['disheartened', 92.79027445159294], ['leaned', 92.64926718281589], ['birder', 92.44448100845747], ['captivated', 92.43721619730242], [\"ravi's\", 92.31017082428738], [\"abuela's\", 91.35242003061113], ['apprehensions', 91.29401570563567], ['gestured', 91.25838311067005], ['sagely', 90.85405765687906], [\"jamie's\", 90.7101431289521], [\"emily's\", 90.30345237120699], ['piquing', 90.28960468143727], ['bated', 89.97164617298913], ['élise', 88.83753859369634], ['cinephiles', 88.7355702396338], [\"alex's\", 88.2556817444544], ['wholeheartedly', 88.16272402677548], ['enthusiasts', 87.63270532947693], ['enchantingly', 87.09964533268402], ['wambui', 86.62912301520548], ['blankly', 86.48383021818377], ['eadric', 86.34210211634532], ['immersing', 85.82226232320427], ['adversity', 85.78100322441296], ['tldr', 85.71508797142025], ['cleanups', 85.15605680775293], ['candidness', 83.51042672731568], ['todayilearned', 83.43565257213649], ['windowpanes', 82.63160794617292], ['chuckled', 82.17901382209192], [\"jake's\", 81.84386060155019], ['cobblestone', 81.79712032621906], ['scrolling', 81.78887253113474], ['curiosity', 81.51530210405123], ['homebrewer', 81.04455356211855], ['worriedly', 80.79276900031643], ['intriguingly', 80.58716425837639], ['brainstorming', 80.17302665340826], ['shimmered', 79.93482088716453], ['supportively', 79.81980356155465], ['aldric', 79.76253783791124], ['captivating', 79.53261479326709], ['grumbled', 79.50446451638021], ['flytraps', 79.31283233215875], [\"evergreen's\", 79.04051980014812], ['jingled', 78.97208486964455], ['csharp', 78.31214402315507], ['etched', 78.2484104971739], ['intricate', 78.21927462240582], ['vibrantly', 78.21788362848538], ['insights', 78.13599417565045], ['etiquettes', 78.13242301857765], [\"jamal's\", 77.58542678683071], ['serendipity', 77.5119644921462], ['aback', 77.48534961672479], ['vibrant', 77.46467368570217], ['tightknit', 76.78236438341992], ['fostered', 76.37582020518892], ['unease', 76.31779012630413], ['stammered', 76.15241103671885], ['passions', 75.86883777669632], [\"johann's\", 75.25688694533872], ['maplewood', 75.12232429540036], ['user1', 74.66719089614045], ['appreciating', 74.64547548366207], ['bibliophiles', 74.58982689947247], ['reverberated', 74.49733463113604], ['insightfulness', 74.4353052101276], [\"amina's\", 74.37594691251186], ['unwavering', 74.36354568386292], ['makena', 74.13148763784136], ['strummed', 73.90269656900801], ['maya', 73.8392870217604], ['dataisbeautiful', 73.65993744752211], ['geocache', 73.44283447399256], ['conlangs', 73.3074975554287], ['geocaching', 72.83344653573383], ['advancements', 72.75299212520173], [\"maria's\", 72.70713211768961], ['shying', 72.68113986332979], ['quaint', 72.62177837440774], ['unforeseen', 72.5794519923886], ['strategizing', 71.9616017940079], ['dialogues', 71.85996322464466], ['insurmountable', 71.76861982097972], ['clinked', 71.75777624573452], ['trusty', 71.45209585957218], ['persevered', 71.26846357550468], ['collaboratively', 71.12793274132686], ['fascinated', 71.00903688990262], ['thrived', 70.72570258119151], [\"anika's\", 70.40087132339981], ['chanoyu', 70.30001047623162], ['profusely', 70.22379980888442], ['eliab', 70.12200110026922], [\"zara's\", 69.93220781961107], ['yumi', 69.79186886111535], ['solidifying', 69.66883391158434], ['naledi', 69.38661708568878], ['murmured', 69.16906099425256], ['prided', 68.88742307155565], ['curveball', 68.78373574046977], ['belongingness', 68.65677493723611], [\"hometown's\", 68.63745320566527], ['glanced', 68.53580433097356], ['dismissiveness', 68.38075885332297], ['kavarna', 68.17328747022798], ['echoed', 68.15970713407852], ['arben', 67.91829009151938], [\"clara's\", 67.56435395996198], ['wonderment', 67.27027413037723], [\"ayla's\", 67.01057148335981], ['aquarist', 66.95076350167123], ['twinkling', 66.8823110049823], ['yearned', 66.52492167836711], ['sqrt', 66.27978481319188], ['paused', 65.93468997276268], ['nurturing', 65.93279047744063], ['avid', 65.90861536910037], ['brimming', 65.8868500882794], ['freydis', 65.8102619080017], ['gesturing', 65.71995864280908], ['seasoned', 65.65967372807557], ['zizzi', 65.54158913859789], [\"claudette's\", 65.2806182475027], ['breadmaking', 65.1574003584379], ['hyperparameters', 64.887639427456], ['sarah', 64.83972349190283], ['naturedly', 64.23986170183434], ['transformative', 64.05481645093263], ['weren', 63.98985703043479], ['blossomed', 63.34656967116295], ['pastimes', 63.26729479311575], [\"meera's\", 63.11493587608736], ['dataframe', 62.85647995521886], ['slipups', 62.82128595748357], ['émile', 62.81655297158125], ['intricacies', 62.69146579035666], ['enthusiast', 62.68374524810508], ['clinking', 62.56300742571657], [\"alexei's\", 62.549039194897794], ['underscored', 62.51084338044547], [\"ramesh's\", 62.37724953745895], ['huddled', 62.3377958443102], ['jaspreet', 62.272673639845316], ['ofrenda', 62.02942100843967], ['gnawed', 62.018344326116726], ['quirks', 61.996773944751006], ['whirred', 61.93374580894079], ['sipped', 61.842944252172124], [\"fatima's\", 61.816051992685814], ['empathized', 61.720377629749926], ['cheerily', 61.5966576060552], ['unexpected', 61.325131664730904], ['amelia', 61.101950116290936], ['cherished', 61.101151929846445], ['reflecting', 60.927032097356665], ['nervously', 60.9048876340286], ['melodia', 60.851913355398096], ['unlikeliest', 60.84620083891452], ['ahmose', 60.58206785157607], ['intertwine', 60.113328945875296], ['buzzed', 60.08346945794633], ['perusing', 59.98845296235552], ['towering', 59.939238608391236], ['hiroshi', 59.523461555713276], ['prioritizing', 59.1951913370659], ['teemed', 58.95817872069236], ['astonishment', 58.87654436049688], [\"gaudí's\", 58.26487683689298], ['jaxon', 58.22494985325537], ['showcasing', 58.126949032849325], ['diligently', 58.0567812007436], ['setbacks', 57.993771930782145], ['millfield', 57.492411928965225], ['exhilarated', 57.45992032748462], ['murmurs', 57.45979798148657], ['gleaming', 57.418567380145646], ['defne', 57.340115496796855], ['coexisted', 57.307181215539096], ['brainteasers', 57.19112616978138], ['yellowed', 57.16422184259698], ['amara', 57.1135530765296], [\"max's\", 57.111830827461304], ['seamlessly', 57.052298418000575], ['ominously', 56.90590327318063], ['amira', 56.74576385203732], ['quietude', 56.67195414671209], ['adorning', 56.665186571935166], ['teeming', 56.6525883033453], ['delve', 56.596902296062844], ['countless', 56.51125106846517], ['obaa', 56.42379925804734], ['peculiar', 56.18036470693872], ['precariously', 56.08031688405674], ['deepened', 56.03360323513692], ['embarked', 56.01649308334308], ['complexities', 55.98561197719851], ['tapestry', 55.94768582220989], [\"humanity's\", 55.811883749711356], ['empathetic', 55.708259804410844], ['lily', 55.625562062443784], ['triumphantly', 55.49247433293487], ['remorsefully', 55.47346594250702], ['grumble', 55.39591018490637], ['nuances', 55.36617782480204], ['researching', 54.99205971726203], ['windowpane', 54.890398554195606], ['deciphering', 54.8627833076834], ['pulsated', 54.8443464082954], ['aquafaba', 54.82600437520151], ['breathwork', 54.7877086993204], [\"lila's\", 54.63315099867143], ['hunched', 54.534772739527725], ['reminiscing', 54.52314396328542], ['shockwaves', 54.474490710921955], ['considerately', 54.12983172465151], ['sparking', 54.01108451941536], ['emboldened', 53.8762318572456], ['delectable', 53.82568959921968], ['vigor', 53.51341918961744], ['homebrewing', 53.459566790168395], ['dismissively', 53.455227623475594], [\"aisha's\", 53.12263235081756], ['pastime', 53.03810874416869], [\"thompson's\", 52.62545446543177], ['enriching', 52.46110086597417], ['symbolizing', 52.232380464377385], ['mindedness', 52.20923836204829], ['hüseyin', 52.1335166949871], ['terrariums', 52.07499895439067], [\"hassan's\", 51.94861311739921], ['thoren', 51.82316166520704], ['brushstroke', 51.81404946715164], ['trudged', 51.791107652399816], ['dimly', 51.73273818885395], ['preserving', 51.50105302476137], ['flocked', 51.465345199302455], ['proactivity', 51.26042430558556], ['indelible', 51.117498051009456], ['wafting', 51.028249764997334], [\"nonna's\", 50.99071588203979], ['enthusiastically', 50.97424487320517], [\"samantha's\", 50.904144208216294], ['sini', 50.83488576920611], ['novák', 50.78232818822808], ['scrolled', 50.76840660448693], ['apiarists', 50.75134446145064], ['enriches', 50.670283286269154], ['meetups', 50.42435427710139], ['thoughtfulness', 50.2945029382233], ['tagines', 50.127841042592706], [\"klara's\", 50.08730817877745], ['basked', 50.006051569637116], ['compromising', 49.977742192136375], ['pemdas', 49.95373144329856], ['ambled', 49.85097801711601], [\"solution's\", 49.766544117434016], ['expectantly', 49.73558995437988], ['memoization', 49.53555181241352], ['buendía', 49.527367161777406], ['bleakest', 49.49804375846876], ['horology', 49.433973211140206], ['scoffed', 49.424866063308336], ['loomed', 49.37974447061713], ['grinned', 49.320505087385264], ['maycomb', 49.23585292544899], ['intertwined', 49.105316327440725], ['coursed', 49.09384736892685], ['surmountable', 48.91773157864695], [\"mei's\", 48.73256875953959], ['chatbot', 48.7303131442302], ['skillset', 48.51810977088149], ['jiyeon', 48.47057359098148], ['nodding', 48.425368506756456], ['dawned', 48.30874496705882], ['greenwashing', 48.23103960043982], ['browsing', 48.04007219005388], ['seemingly', 47.816609926836044], ['ventured', 47.77434558569385], ['factorials', 47.6998395729457], ['nalini', 47.66197815287227], ['jsem', 47.60235342557785], ['nonno', 47.560413832291594], ['skillsets', 47.53934826086816], ['mariama', 47.51074430078741], ['marianne', 47.49747840168747], ['grueling', 47.442666916537256], ['aficionados', 47.392106431743535], ['confidently', 47.36682440819566], ['overwatering', 47.36418703492115], ['smiled', 47.303430838646], ['townsfolk', 47.24488793502313], ['gamifying', 47.23204105702876], ['perspectives', 47.06114618839279], ['verdant', 47.05816833710129], [\"mira's\", 47.03448271248642], ['comfortingly', 46.929983660664995], ['lighthearted', 46.887045498727474], ['thrummed', 46.7836356039557], ['immortalizing', 46.61875430226617], ['puzzled', 46.57402757497557], ['nonchalantly', 46.567771645493856], ['nums', 46.55301856122418], ['learnings', 46.40542923819422], ['ravi', 46.374435734329666], ['incorporating', 46.372182420668544], ['aquarists', 46.36597421549888], ['scouring', 46.31445449051849], ['vermicomposting', 46.25759923265036], ['armfuls', 46.25505169238614], ['predicament', 46.236275753777775], ['regaling', 46.218313436958525], ['pièce', 46.102628719034584], ['revolutionize', 45.98587923574832], ['introspection', 45.8887810915839], ['fprintf', 45.85312101996422], ['tifu', 45.74958040176368], ['preconceived', 45.72185195492253], ['exploring', 45.705327066842976], ['lockdowns', 45.686017038389906], ['luminara', 45.6720329499178], ['peasy', 45.65691856858046], ['elior', 45.65086602306514], ['jamie', 45.62163326920453], ['honing', 45.61424457042017], ['coursing', 45.613144852573434], ['sipping', 45.60713179645527], [\"samira's\", 45.25193951663313], ['upvoting', 45.20629015918102], ['quirky', 45.149367477847996], ['ayla', 45.02322254830481], ['zucchinis', 44.99761388408844], ['nonna', 44.97515920772422], ['gadgetry', 44.97133023111876], ['ttrpg', 44.953641232732316], ['makeshift', 44.79510094891597]];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_penalties = {}\n",
    "for el in word_penalties_list[:500]:\n",
    "    word_penalties[el[0]] = el[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the over-represented words and their penalty values from a JSON file\n",
    "if os.path.exists('over_represented_words.json'):\n",
    "    with open('over_represented_words.json', 'r') as f:\n",
    "        over_represented_words = json.load(f)\n",
    "        # Use the top 500 words\n",
    "        for el in over_represented_words[:500]:\n",
    "            word_penalties[el[0]] = el[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precompute_starting_tokens(\n",
    "    tokenizer: PreTrainedTokenizer, word_penalties: Dict[str, float]\n",
    ") -> Dict[Tuple[int, ...], Set[int]]:\n",
    "    \"\"\"\n",
    "    Precompute all starting token IDs for each target word variant.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer used by the model.\n",
    "        word_penalties (Dict[str, float]): Dictionary of target words with their respective penalty.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[int, ...], Set[int]]: A mapping from each token sequence (word variant) to a set of starting token IDs.\n",
    "    \"\"\"\n",
    "    starting_tokens_lookup = {}\n",
    "\n",
    "    for word in word_penalties.keys():\n",
    "        variants = [\n",
    "            word.lower(),\n",
    "            word.capitalize(),\n",
    "            word.upper(),\n",
    "            f\" {word.lower()}\",\n",
    "            f\" {word.capitalize()}\",\n",
    "            f\" {word.upper()}\",\n",
    "        ]\n",
    "\n",
    "        for variant in variants:\n",
    "            # Encode the full variant\n",
    "            token_ids = tokenizer.encode(variant, add_special_tokens=False)\n",
    "            starting_tokens = set()\n",
    "            if token_ids:\n",
    "                starting_tokens.add(token_ids[0])\n",
    "                first_token_decoded = tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Iterate over all possible prefixes of the first token\n",
    "                for i in range(len(first_token_decoded) - 1):\n",
    "                    prefix = first_token_decoded[:-(i + 1)]\n",
    "                    encoded_prefix = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "                    if encoded_prefix:\n",
    "                        starting_tokens.add(encoded_prefix[0])  # Add the first token of the prefix\n",
    "\n",
    "                starting_tokens_lookup[tuple(token_ids)] = starting_tokens\n",
    "\n",
    "    return starting_tokens_lookup\n",
    "\n",
    "# Precompute starting tokens\n",
    "starting_tokens_lookup = precompute_starting_tokens(tokenizer, word_penalties)\n",
    "\n",
    "class AdvancedCustomWordSampler:\n",
    "    \"\"\"\n",
    "    A sampler that generates text while downregulating specified words or phrases.\n",
    "    It uses backtracking and custom adjustments to avoid overrepresented words.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        word_penalties: Dict[str, float],\n",
    "        starting_tokens_lookup: Dict[Tuple[int, ...], Set[int]],\n",
    "        max_backtrack: int = 5,\n",
    "        adjustment_strength: float = 1.0,\n",
    "        device: torch.device = torch.device('cuda'),\n",
    "        output_every_n_tokens: int = 5,\n",
    "        slow_debug: bool = False,\n",
    "        debug_delay: float = 2.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the AdvancedCustomWordSampler with the necessary parameters.\n",
    "\n",
    "        Args:\n",
    "            model (PreTrainedModel): The language model to use for Generation.\n",
    "            tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "            word_penalties (Dict[str, float]): Dictionary of target words with their respective penalty.\n",
    "            starting_tokens_lookup (Dict[Tuple[int, ...], Set[int]]): Mapping from token sequences to starting token IDs.\n",
    "            max_backtrack (int): Maximum number of tokens to backtrack.\n",
    "            adjustment_strength (float): Strength of the downregulation adjustment.\n",
    "            device (torch.device): Device to run the model on.\n",
    "            output_every_n_tokens (int): Frequency of updating the inference output display.\n",
    "            slow_debug (bool): Enables slow debug mode when set to True.\n",
    "            debug_delay (float): Time in seconds to pause during slow debug steps.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_penalties = word_penalties\n",
    "        self.starting_tokens_lookup = starting_tokens_lookup\n",
    "        self.max_backtrack = max_backtrack\n",
    "        self.adjustment_strength = adjustment_strength\n",
    "        self.device = device\n",
    "        self.output_every_n_tokens = output_every_n_tokens\n",
    "        self.slow_debug = slow_debug\n",
    "        self.debug_delay = debug_delay\n",
    "\n",
    "        # Prepare token sequences for downregulation\n",
    "        self.token_sequences = self._prepare_token_sequences()\n",
    "        self.max_sequence_length = max(len(seq) for seq in self.token_sequences.keys())\n",
    "\n",
    "        # Initialize a cache to store logits along with their positions\n",
    "        self.logit_cache = {}\n",
    "\n",
    "        # Record of downregulated sequences at specific positions\n",
    "        self.downregulated_positions = {}  # Key: position, Value: set of sequences\n",
    "\n",
    "    def _prepare_token_sequences(self) -> Dict[Tuple[int, ...], float]:\n",
    "        \"\"\"\n",
    "        Prepares the token sequences from word_penalties for efficient lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[Tuple[int, ...], float]: Mapping from token ID sequences to their adjustment factors.\n",
    "        \"\"\"\n",
    "        token_sequences = {}\n",
    "        for word, penalty in self.word_penalties.items():\n",
    "            variants = [\n",
    "                word.lower(),\n",
    "                word.capitalize(),\n",
    "                word.upper(),\n",
    "                f\" {word.lower()}\",\n",
    "                f\" {word.capitalize()}\",\n",
    "                f\" {word.upper()}\",\n",
    "            ]\n",
    "            for variant in variants:\n",
    "                token_ids = tuple(self.tokenizer.encode(variant, add_special_tokens=False))\n",
    "                if token_ids:\n",
    "                    token_sequences[token_ids] = 1 / penalty  # Inverse of the over-representation penalty\n",
    "        return token_sequences\n",
    "\n",
    "    def _adjust_logits(self, logits: torch.FloatTensor, adjustment: float) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Adjusts the logits by applying the downregulation factor.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.FloatTensor): The original logits.\n",
    "            adjustment (float): The adjustment factor to apply.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The adjusted logits.\n",
    "        \"\"\"\n",
    "        log_adjustment = torch.log(torch.tensor(adjustment ** self.adjustment_strength, device=self.device))\n",
    "        return logits + log_adjustment  # Lowering the logit for disallowed tokens\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Generates text in a streaming fashion with custom downregulation and backtracking.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The initial text prompt.\n",
    "            max_length (int): The maximum length of the generated text.\n",
    "            temperature (float): Sampling temperature.\n",
    "            top_k (int): Top-k filtering.\n",
    "            top_p (float): Top-p (nucleus) filtering.\n",
    "\n",
    "        Yields:\n",
    "            Generator[str, None, None]: Yields generated text chunks.\n",
    "        \"\"\"\n",
    "        # Encode the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        generated_sequence = input_ids[0].tolist()\n",
    "        current_position = len(generated_sequence)  # Tracks the current position in the sequence\n",
    "\n",
    "        # Initialize display (if using in Jupyter)\n",
    "        with inference_output:\n",
    "            inference_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<div style='white-space: pre-wrap;'>{self.tokenizer.decode(generated_sequence)}</div>\"))\n",
    "\n",
    "        past_key_values = None\n",
    "        output_tokens_counter = 0\n",
    "\n",
    "        while len(generated_sequence) < max_length:\n",
    "            current_input_ids = torch.tensor([generated_sequence], device=self.device)\n",
    "\n",
    "            regenerating = False\n",
    "            if current_position in self.logit_cache:\n",
    "                # We backtracked and want to use the cached logits\n",
    "                next_token_logits = self.logit_cache[current_position]\n",
    "                past_key_values = None\n",
    "                regenerating = True\n",
    "            else:\n",
    "                if past_key_values is None:\n",
    "                    outputs = self.model(current_input_ids, use_cache=True)\n",
    "                else:\n",
    "                    outputs = self.model(current_input_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "                next_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "                past_key_values = outputs.past_key_values\n",
    "                self.logit_cache[current_position] = next_token_logits.clone()\n",
    "\n",
    "            # Apply top-k and top-p filtering\n",
    "            filtered_logits = self._filter_logits(next_token_logits, top_k, top_p)\n",
    "\n",
    "            # Sample the next token\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = next_token_index.item()\n",
    "\n",
    "            if regenerating:\n",
    "                with debug_output:\n",
    "                    debug_output.clear_output(wait=True)\n",
    "                alt_token = tokenizer.decode(next_token, skip_special_tokens=True)\n",
    "                debug_info = f\"Alternate token: {alt_token}\"\n",
    "                self._display_debug(debug_info)\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "\n",
    "            # Append the new token to the sequence\n",
    "            generated_sequence.append(next_token)\n",
    "            current_position += 1\n",
    "            output_tokens_counter += 1\n",
    "\n",
    "            # Yield the current text chunk\n",
    "            current_text = self.tokenizer.decode(generated_sequence)\n",
    "            if output_tokens_counter >= self.output_every_n_tokens:\n",
    "                output_tokens_counter = 0\n",
    "                with inference_output:\n",
    "                    inference_output.clear_output(wait=True)\n",
    "                    display(HTML(f\"<div style='white-space: pre-wrap;'>{current_text}</div>\"))\n",
    "                yield current_text  # Yield the generated text chunk\n",
    "\n",
    "            # Check for end-of-sequence token\n",
    "            if next_token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # After adding the token, check for disallowed sequences\n",
    "            matched_sequence, start_pos = self._detect_disallowed_sequence(generated_sequence)\n",
    "\n",
    "            if matched_sequence:\n",
    "                # Downregulate the relevant tokens at the start_pos\n",
    "                adjustment = self.token_sequences[matched_sequence]\n",
    "                word = self.tokenizer.decode(torch.tensor(matched_sequence))\n",
    "\n",
    "                # Display debug information\n",
    "                debug_info = f\"Replacing '{word}'\"\n",
    "                self._display_debug(debug_info)\n",
    "\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "                    with debug_output:\n",
    "                        debug_output.clear_output(wait=True)\n",
    "\n",
    "                # Identify starting tokens to downregulate\n",
    "                starting_tokens = self.starting_tokens_lookup.get(matched_sequence, set())\n",
    "\n",
    "                # Find the cached logits corresponding to start_pos\n",
    "                for token_id in starting_tokens:\n",
    "                    # Apply downregulation\n",
    "                    self.logit_cache[start_pos][:, token_id] = self._adjust_logits(\n",
    "                        self.logit_cache[start_pos][:, token_id], adjustment\n",
    "                    )\n",
    "\n",
    "                # Record that this sequence has been downregulated at this position\n",
    "                if start_pos not in self.downregulated_positions:\n",
    "                    self.downregulated_positions[start_pos] = set()\n",
    "                self.downregulated_positions[start_pos].add(matched_sequence)\n",
    "\n",
    "                # Backtrack: remove tokens from the generated_sequence that are part of the disallowed sequence\n",
    "                for _ in range(len(matched_sequence)):\n",
    "                    generated_sequence.pop()\n",
    "                    current_position -= 1\n",
    "\n",
    "                # Update the model's past_key_values by re-encoding up to start_pos\n",
    "                # This is necessary because we've modified the generated_sequence\n",
    "                new_input_ids = torch.tensor([generated_sequence], device=self.device)\n",
    "                outputs = self.model(new_input_ids, use_cache=True)\n",
    "                past_key_values = outputs.past_key_values\n",
    "\n",
    "                # Clear the logit_cache ahead of start_pos since we've backtracked\n",
    "                to_del = [key for key in self.logit_cache if key > start_pos]\n",
    "                for key in to_del:\n",
    "                    del self.logit_cache[key]\n",
    "\n",
    "                continue  # Continue to the next iteration\n",
    "\n",
    "        # Final display of the generated text\n",
    "        final_text = self.tokenizer.decode(generated_sequence)\n",
    "        with inference_output:\n",
    "            inference_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<div style='white-space: pre-wrap;'>{final_text}</div>\"))\n",
    "        yield final_text\n",
    "\n",
    "        # Clear variables to free up memory\n",
    "        del outputs, next_token_logits, filtered_logits, past_key_values\n",
    "\n",
    "    def _filter_logits(self, logits: torch.FloatTensor, top_k: int, top_p: float) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Applies top-k and top-p (nucleus) filtering to the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.FloatTensor): The original logits.\n",
    "            top_k (int): The number of top tokens to keep.\n",
    "            top_p (float): The cumulative probability threshold.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The filtered logits.\n",
    "        \"\"\"\n",
    "        # Apply top-k\n",
    "        if top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            top_k_logits, _ = torch.topk(logits, top_k)\n",
    "            min_top_k = top_k_logits[:, -1].unsqueeze(-1)\n",
    "            logits = torch.where(logits < min_top_k, float('-inf'), logits)\n",
    "\n",
    "        # Apply top-p\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift the mask right to keep the first token above the threshold\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = False\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                dim=1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "            )\n",
    "            logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _detect_disallowed_sequence(self, generated_sequence: List[int]) -> Tuple[Tuple[int, ...], int]:\n",
    "        \"\"\"\n",
    "        Detects if the recent tokens in the generated_sequence match any disallowed sequence.\n",
    "\n",
    "        Args:\n",
    "            generated_sequence (List[int]): The list of generated token IDs.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[int, ...], int]: The matched disallowed sequence and its start position.\n",
    "                                         Returns (None, -1) if no match is found.\n",
    "        \"\"\"\n",
    "        # Start checking from the longest possible sequence to the shortest\n",
    "        for seq_length in range(self.max_sequence_length, 0, -1):\n",
    "            if len(generated_sequence) < seq_length:\n",
    "                continue\n",
    "            candidate_sequence = tuple(generated_sequence[-seq_length:])\n",
    "            if candidate_sequence in self.token_sequences:\n",
    "                start_pos = len(generated_sequence) - seq_length\n",
    "                return candidate_sequence, start_pos\n",
    "        return None, -1\n",
    "\n",
    "    def _display_debug(self, message: str):\n",
    "        \"\"\"\n",
    "        Displays debug information in the debug_output widget.\n",
    "\n",
    "        Args:\n",
    "            message (str): The debug message to display.\n",
    "        \"\"\"\n",
    "        with debug_output:\n",
    "            debug_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<pre>{message}</pre>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Prompt</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa1052e4234426fb9e806033a44b68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Inference Output</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f20ffef8fc40819a63bed3021f1055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Debug Information</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8efe81c23a4c6d88280b42072e0e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup separate output widgets for prompt, inference, and debug\n",
    "prompt_output = Output()\n",
    "inference_output = Output()\n",
    "debug_output = Output()\n",
    "\n",
    "# Display the output widgets\n",
    "display(HTML(\"<h2>Prompt</h2>\"))\n",
    "display(prompt_output)\n",
    "display(HTML(\"<h2>Inference Output</h2>\"))\n",
    "display(inference_output)\n",
    "display(HTML(\"<h2>Debug Information</h2>\"))\n",
    "display(debug_output)\n",
    "\n",
    "# Enable slow debug mode\n",
    "SLOW_DEBUG = True\n",
    "\n",
    "# Initialize the sampler\n",
    "sampler = AdvancedCustomWordSampler(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    word_penalties=word_penalties,\n",
    "    starting_tokens_lookup=starting_tokens_lookup,\n",
    "    max_backtrack=5,\n",
    "    adjustment_strength=1.0,\n",
    "    device=device,\n",
    "    output_every_n_tokens=5,\n",
    "    slow_debug=SLOW_DEBUG,          # Enable slow debug\n",
    "    debug_delay=1.5                # Set delay to 1.5 seconds per debug step\n",
    ")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Write a story about Elara, the weaver of tapestries in future Technopolis. In the bustling city, a group of \"\n",
    "\n",
    "# Display the prompt\n",
    "with prompt_output:\n",
    "    prompt_output.clear_output(wait=True)\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap;'>{prompt}</div>\"))\n",
    "\n",
    "# Start generating\n",
    "try:\n",
    "    for text_chunk in sampler.generate_stream(prompt, max_length=300, temperature=1.0, top_k=50, top_p=0.95):\n",
    "        pass  # The text_chunk is already being displayed via the inference_output widget\n",
    "    print(\"\\nGeneration complete.\")\n",
    "except Exception as e:\n",
    "    with debug_output:\n",
    "        debug_output.clear_output(wait=True)\n",
    "        debug_output.append_stdout(f\"\\n\\nAn error occurred: {str(e)}\\n\")\n",
    "        traceback.print_exc(file=sys.stdout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
