{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Dict, Tuple, Generator, Set\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from ipywidgets import Output\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are automatically derived from relative frequencey of words in a gpt generated dataset. See slopcalc.ipynb for the code to generate a more complete list.\n",
    "word_penalties_list = [['a kaleidoscope of', 2000], ['testament', 2000.0150485530914], ['technopolis', 1682.682059035117], ['understandingly', 762.9294022671543], ['paperbound', 659.2264970913199], ['hesitantly', 496.5646879026894], ['piqued', 482.3001178804444], ['delved', 473.4940223966827], ['curveballs', 462.50687039417824], ['bustling', 454.70303449492854], ['marveled', 428.19439049963717], ['inclusivity', 399.28185144068397], ['birdwatcher', 382.93952575702605], ['elara', 382.02399833524635], ['camaraderie', 325.065910926091], ['newfound', 289.3537643476301], ['marveling', 281.4117889244332], [\"hiroshi's\", 277.20734354116485], ['greentech', 268.92005660614404], ['thoughtfully', 266.9326102346037], ['intently', 251.51633784078055], ['birdwatching', 250.1588231011304], ['amidst', 249.22122673588677], ['cherishing', 247.91009553317267], ['attentively', 246.79285812826976], ['interjected', 235.9610251843368], ['serendipitous', 233.0266906486917], [\"marianne's\", 232.87022859034334], [\"maya's\", 230.37564440034032], ['excitedly', 228.84649139211055], ['steepled', 228.70847810531137], ['engrossed', 228.4472196666415], ['fostering', 222.59645412759878], ['brainstormed', 218.63487421031408], ['furrowed', 217.1288191216257], ['nodded', 215.746532494257], ['contemplatively', 213.9394730581084], ['jotted', 212.19052857841066], [\"mia's\", 209.54359039341304], ['yesteryears', 205.40375361782048], ['conspiratorially', 204.18903883197223], ['poring', 203.12158920489887], ['stumbled', 201.95286430826962], ['strategized', 198.31538808865406], ['hesitated', 194.35575102206053], ['intrigued', 191.32480777377384], [\"sarah's\", 188.1056806342427], ['lykos', 186.8984432180823], ['adaptability', 185.2743410645729], ['yoing', 184.27349339389698], ['geocaches', 182.61995131301913], ['furrowing', 181.28300434012144], ['quandaries', 178.45513005800902], ['chimed', 177.6317240627814], ['headfirst', 177.55430176035384], ['gruffly', 173.5169670342752], ['skeptically', 173.2196196510284], ['nestled', 170.24886793134038], ['fruitville', 168.93895251122095], ['gastronomical', 168.77834340202367], ['sighed', 167.83599102428747], ['warmly', 166.64524795750117], ['approvingly', 165.14554435242388], ['questioningly', 164.2217764827755], [\"timmy's\", 162.85237720972512], ['undeterred', 159.81034467083455], ['starlit', 158.81973280586473], ['unearthing', 157.53953848282245], ['grappled', 155.11380760257956], [\"yumi's\", 153.362396079487], [\"seabrook's\", 152.65396517679832], ['geocachers', 152.48899331241418], ['animatedly', 150.64516344395025], ['bakersville', 149.48324667712868], ['minji', 148.7787149817242], ['fateful', 147.881376001738], ['sparkled', 145.48284973440963], ['resonated', 144.91492949803347], ['harmoniously', 144.8378436549682], ['fidgeted', 143.88462648395776], ['mwanga', 141.271194443305], ['gleamed', 140.84454272803274], ['embracing', 140.8134127640521], ['pleasantries', 138.9683910665212], ['iostream', 137.02499195670802], ['navigated', 136.8749045617025], ['interconnectedness', 136.6775722710472], ['tanginess', 136.0248012468762], ['mouthwatering', 135.40207079890078], [\"amelia's\", 135.12735430462644], ['delving', 134.62133115310917], ['mischievously', 134.53400914082113], ['tirelessly', 134.50459651470058], ['transcended', 132.75875026522667], ['sympathetically', 132.28731274201124], ['pondered', 132.24181930810568], ['lingered', 131.6820547398057], ['empathizing', 130.38734974729505], ['niche', 128.82354262722254], ['regaled', 128.21211629309659], ['greenthumb', 127.87603715586023], ['savored', 127.44044593637169], [\"amira's\", 127.26977675143385], ['meticulously', 125.67264078678225], ['firsthand', 123.40718461639742], ['empathetically', 122.76583436768932], ['unshed', 122.234447281337], [\"jenkin's\", 122.13793091468249], ['empathy', 120.78510640297107], ['enigmatically', 120.10278606401909], [\"marla's\", 119.96311948139385], ['bayville', 119.86205591147561], ['adversities', 119.16540991510242], ['eagerly', 118.7736944560049], ['labyrinthine', 117.30247757246565], ['quizzically', 116.99368259297609], ['transcending', 116.98548707285242], ['resilience', 115.03290831887757], [\"lily's\", 114.79275367950875], ['commiserated', 114.67810631179985], ['savoring', 114.09168105940152], [\"amara's\", 113.35572623503091], ['somberly', 111.33701848917218], ['cinephile', 110.08462885614495], ['solace', 109.58942409221955], ['twinkled', 108.8491511689895], ['aquascaping', 108.490673606918], ['rippled', 107.44694800406151], ['reveled', 107.0468756808211], ['greenhaven', 106.94331659047654], ['birdwatchers', 106.78153731714639], ['adwoa', 105.93106020380063], ['appreciatively', 105.79259888560438], ['awestruck', 105.76861040590829], ['ecotech', 105.3920442928442], ['navigating', 105.00722107822565], ['lightheartedness', 103.97167324805294], ['disapprovingly', 103.7632507571004], ['exclaimed', 103.50051962551075], [\"samir's\", 103.32935174824694], ['fishkeeping', 102.85891331779236], ['sparked', 101.88166406410981], ['welled', 101.1238612235024], ['jotting', 101.00457387540926], ['resourcefulness', 100.7218012566337], ['flickered', 99.7346200354375], ['reminisced', 99.40737436442414], [\"abernathy's\", 97.97170234043274], ['unbeknownst', 96.90280381444087], ['pattered', 96.45923446705675], ['reassuringly', 95.99564132547866], ['miscommunications', 95.92537976660182], ['wafted', 95.46404472730974], ['absentmindedly', 94.91609082523561], ['weightiness', 94.72530273625273], ['allyship', 94.20061224332532], ['perseverance', 93.51317590165482], ['timmy', 93.37738761768523], ['mindfully', 92.98616009115628], ['disheartened', 92.79027445159294], ['leaned', 92.64926718281589], ['birder', 92.44448100845747], ['captivated', 92.43721619730242], [\"ravi's\", 92.31017082428738], [\"abuela's\", 91.35242003061113], ['apprehensions', 91.29401570563567];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_penalties = {}\n",
    "for el in word_penalties_list[:500]:\n",
    "    word_penalties[el[0]] = el[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your own words/phrases + penalty values from a JSON file\n",
    "if os.path.exists('slop_phrases.json'):\n",
    "    with open('slop_phrases.json', 'r') as f:\n",
    "        slop_phrases = json.load(f)\n",
    "        # Use the top 500 words\n",
    "        for el in slop_phrases[:500]:\n",
    "            word_penalties[el[0]] = el[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precompute_starting_tokens(\n",
    "    tokenizer: PreTrainedTokenizer, word_penalties: Dict[str, float]\n",
    ") -> Dict[Tuple[int, ...], Set[int]]:\n",
    "    \"\"\"\n",
    "    Precompute all starting token IDs for each target word variant.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer used by the model.\n",
    "        word_penalties (Dict[str, float]): Dictionary of target words with their respective penalty.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[int, ...], Set[int]]: A mapping from each token sequence (word variant) to a set of starting token IDs.\n",
    "    \"\"\"\n",
    "    starting_tokens_lookup = {}\n",
    "\n",
    "    for word in word_penalties.keys():\n",
    "        variants = [\n",
    "            word.lower(),\n",
    "            word.capitalize(),\n",
    "            word.upper(),\n",
    "            f\" {word.lower()}\",\n",
    "            f\" {word.capitalize()}\",\n",
    "            f\" {word.upper()}\",\n",
    "        ]\n",
    "\n",
    "        for variant in variants:\n",
    "            # Encode the full variant\n",
    "            token_ids = tokenizer.encode(variant, add_special_tokens=False)\n",
    "            starting_tokens = set()\n",
    "            if token_ids:\n",
    "                starting_tokens.add(token_ids[0])\n",
    "                first_token_decoded = tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Iterate over all possible prefixes of the first token\n",
    "                for i in range(len(first_token_decoded) - 1):\n",
    "                    prefix = first_token_decoded[:-(i + 1)]\n",
    "                    encoded_prefix = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "                    if encoded_prefix:\n",
    "                        starting_tokens.add(encoded_prefix[0])  # Add the first token of the prefix\n",
    "\n",
    "                starting_tokens_lookup[tuple(token_ids)] = starting_tokens\n",
    "\n",
    "    return starting_tokens_lookup\n",
    "\n",
    "# Precompute starting tokens\n",
    "starting_tokens_lookup = precompute_starting_tokens(tokenizer, word_penalties)\n",
    "\n",
    "class AdvancedCustomWordSampler:\n",
    "    \"\"\"\n",
    "    A sampler that generates text while downregulating specified words or phrases.\n",
    "    It uses backtracking and custom adjustments to avoid overrepresented words.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        word_penalties: Dict[str, float],\n",
    "        starting_tokens_lookup: Dict[Tuple[int, ...], Set[int]],\n",
    "        max_backtrack: int = 5,\n",
    "        adjustment_strength: float = 1.0,\n",
    "        device: torch.device = torch.device('cuda'),\n",
    "        output_every_n_tokens: int = 5,\n",
    "        slow_debug: bool = False,\n",
    "        debug_delay: float = 2.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the AdvancedCustomWordSampler with the necessary parameters.\n",
    "\n",
    "        Args:\n",
    "            model (PreTrainedModel): The language model to use for Generation.\n",
    "            tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
    "            word_penalties (Dict[str, float]): Dictionary of target words with their respective penalty.\n",
    "            starting_tokens_lookup (Dict[Tuple[int, ...], Set[int]]): Mapping from token sequences to starting token IDs.\n",
    "            max_backtrack (int): Maximum number of tokens to backtrack.\n",
    "            adjustment_strength (float): Strength of the downregulation adjustment.\n",
    "            device (torch.device): Device to run the model on.\n",
    "            output_every_n_tokens (int): Frequency of updating the inference output display.\n",
    "            slow_debug (bool): Enables slow debug mode when set to True.\n",
    "            debug_delay (float): Time in seconds to pause during slow debug steps.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_penalties = word_penalties\n",
    "        self.starting_tokens_lookup = starting_tokens_lookup\n",
    "        self.max_backtrack = max_backtrack\n",
    "        self.adjustment_strength = adjustment_strength\n",
    "        self.device = device\n",
    "        self.output_every_n_tokens = output_every_n_tokens\n",
    "        self.slow_debug = slow_debug\n",
    "        self.debug_delay = debug_delay\n",
    "\n",
    "        # Prepare token sequences for downregulation\n",
    "        self.token_sequences = self._prepare_token_sequences()\n",
    "        self.max_sequence_length = max(len(seq) for seq in self.token_sequences.keys())\n",
    "\n",
    "        # Initialize a cache to store logits along with their positions\n",
    "        self.logit_cache = {}\n",
    "\n",
    "        # Record of downregulated sequences at specific positions\n",
    "        self.downregulated_positions = {}  # Key: position, Value: set of sequences\n",
    "\n",
    "    def _prepare_token_sequences(self) -> Dict[Tuple[int, ...], float]:\n",
    "        \"\"\"\n",
    "        Prepares the token sequences from word_penalties for efficient lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[Tuple[int, ...], float]: Mapping from token ID sequences to their adjustment factors.\n",
    "        \"\"\"\n",
    "        token_sequences = {}\n",
    "        for word, penalty in self.word_penalties.items():\n",
    "            variants = [\n",
    "                word.lower(),\n",
    "                word.capitalize(),\n",
    "                word.upper(),\n",
    "                f\" {word.lower()}\",\n",
    "                f\" {word.capitalize()}\",\n",
    "                f\" {word.upper()}\",\n",
    "            ]\n",
    "            for variant in variants:\n",
    "                token_ids = tuple(self.tokenizer.encode(variant, add_special_tokens=False))\n",
    "                if token_ids:\n",
    "                    token_sequences[token_ids] = 1 / penalty  # Inverse of the over-representation penalty\n",
    "        return token_sequences\n",
    "\n",
    "    def _adjust_logits(self, logits: torch.FloatTensor, adjustment: float) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Adjusts the logits by applying the downregulation factor.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.FloatTensor): The original logits.\n",
    "            adjustment (float): The adjustment factor to apply.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The adjusted logits.\n",
    "        \"\"\"\n",
    "        log_adjustment = torch.log(torch.tensor(adjustment ** self.adjustment_strength, device=self.device))\n",
    "        return logits + log_adjustment  # Lowering the logit for disallowed tokens\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Generates text in a streaming fashion with custom downregulation and backtracking.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The initial text prompt.\n",
    "            max_length (int): The maximum length of the generated text.\n",
    "            temperature (float): Sampling temperature.\n",
    "            top_k (int): Top-k filtering.\n",
    "            top_p (float): Top-p (nucleus) filtering.\n",
    "\n",
    "        Yields:\n",
    "            Generator[str, None, None]: Yields generated text chunks.\n",
    "        \"\"\"\n",
    "        # Encode the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        generated_sequence = input_ids[0].tolist()\n",
    "        current_position = len(generated_sequence)  # Tracks the current position in the sequence\n",
    "\n",
    "        # Initialize display (if using in Jupyter)\n",
    "        with inference_output:\n",
    "            inference_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<div style='white-space: pre-wrap;'>{self.tokenizer.decode(generated_sequence)}</div>\"))\n",
    "\n",
    "        past_key_values = None\n",
    "        output_tokens_counter = 0\n",
    "\n",
    "        while len(generated_sequence) < max_length:\n",
    "            current_input_ids = torch.tensor([generated_sequence], device=self.device)\n",
    "\n",
    "            regenerating = False\n",
    "            if current_position in self.logit_cache:\n",
    "                # We backtracked and want to use the cached logits\n",
    "                next_token_logits = self.logit_cache[current_position]\n",
    "                past_key_values = None\n",
    "                regenerating = True\n",
    "            else:\n",
    "                if past_key_values is None:\n",
    "                    outputs = self.model(current_input_ids, use_cache=True)\n",
    "                else:\n",
    "                    outputs = self.model(current_input_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "                next_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "                past_key_values = outputs.past_key_values\n",
    "                self.logit_cache[current_position] = next_token_logits.clone()\n",
    "\n",
    "            # Apply top-k and top-p filtering\n",
    "            filtered_logits = self._filter_logits(next_token_logits, top_k, top_p)\n",
    "\n",
    "            # Sample the next token\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = next_token_index.item()\n",
    "\n",
    "            if regenerating:\n",
    "                with debug_output:\n",
    "                    debug_output.clear_output(wait=True)\n",
    "                alt_token = tokenizer.decode(next_token, skip_special_tokens=True)\n",
    "                debug_info = f\"Alternate token: {alt_token}\"\n",
    "                self._display_debug(debug_info)\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "\n",
    "            # Append the new token to the sequence\n",
    "            generated_sequence.append(next_token)\n",
    "            current_position += 1\n",
    "            output_tokens_counter += 1\n",
    "\n",
    "            # Yield the current text chunk\n",
    "            current_text = self.tokenizer.decode(generated_sequence)\n",
    "            if output_tokens_counter >= self.output_every_n_tokens:\n",
    "                output_tokens_counter = 0\n",
    "                with inference_output:\n",
    "                    inference_output.clear_output(wait=True)\n",
    "                    display(HTML(f\"<div style='white-space: pre-wrap;'>{current_text}</div>\"))\n",
    "                yield current_text  # Yield the generated text chunk\n",
    "\n",
    "            # Check for end-of-sequence token\n",
    "            if next_token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # After adding the token, check for disallowed sequences\n",
    "            matched_sequence, start_pos = self._detect_disallowed_sequence(generated_sequence)\n",
    "\n",
    "            if matched_sequence:\n",
    "                # Downregulate the relevant tokens at the start_pos\n",
    "                adjustment = self.token_sequences[matched_sequence]\n",
    "                word = self.tokenizer.decode(torch.tensor(matched_sequence))\n",
    "\n",
    "                # Display debug information\n",
    "                debug_info = f\"Replacing '{word}'\"\n",
    "                self._display_debug(debug_info)\n",
    "\n",
    "                if self.slow_debug:\n",
    "                    time.sleep(self.debug_delay)\n",
    "                    with debug_output:\n",
    "                        debug_output.clear_output(wait=True)\n",
    "\n",
    "                # Identify starting tokens to downregulate\n",
    "                starting_tokens = self.starting_tokens_lookup.get(matched_sequence, set())\n",
    "\n",
    "                # Find the cached logits corresponding to start_pos\n",
    "                for token_id in starting_tokens:\n",
    "                    # Apply downregulation\n",
    "                    self.logit_cache[start_pos][:, token_id] = self._adjust_logits(\n",
    "                        self.logit_cache[start_pos][:, token_id], adjustment\n",
    "                    )\n",
    "\n",
    "                # Record that this sequence has been downregulated at this position\n",
    "                if start_pos not in self.downregulated_positions:\n",
    "                    self.downregulated_positions[start_pos] = set()\n",
    "                self.downregulated_positions[start_pos].add(matched_sequence)\n",
    "\n",
    "                # Backtrack: remove tokens from the generated_sequence that are part of the disallowed sequence\n",
    "                for _ in range(len(matched_sequence)):\n",
    "                    generated_sequence.pop()\n",
    "                    current_position -= 1\n",
    "\n",
    "                # Update the model's past_key_values by re-encoding up to start_pos\n",
    "                # This is necessary because we've modified the generated_sequence\n",
    "                new_input_ids = torch.tensor([generated_sequence], device=self.device)\n",
    "                outputs = self.model(new_input_ids, use_cache=True)\n",
    "                past_key_values = outputs.past_key_values\n",
    "\n",
    "                # Clear the logit_cache ahead of start_pos since we've backtracked\n",
    "                to_del = [key for key in self.logit_cache if key > start_pos]\n",
    "                for key in to_del:\n",
    "                    del self.logit_cache[key]\n",
    "\n",
    "                continue  # Continue to the next iteration\n",
    "\n",
    "        # Final display of the generated text\n",
    "        final_text = self.tokenizer.decode(generated_sequence)\n",
    "        with inference_output:\n",
    "            inference_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<div style='white-space: pre-wrap;'>{final_text}</div>\"))\n",
    "        yield final_text\n",
    "\n",
    "        # Clear variables to free up memory\n",
    "        del outputs, next_token_logits, filtered_logits, past_key_values\n",
    "\n",
    "    def _filter_logits(self, logits: torch.FloatTensor, top_k: int, top_p: float) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Applies top-k and top-p (nucleus) filtering to the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.FloatTensor): The original logits.\n",
    "            top_k (int): The number of top tokens to keep.\n",
    "            top_p (float): The cumulative probability threshold.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The filtered logits.\n",
    "        \"\"\"\n",
    "        # Apply top-k\n",
    "        if top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            top_k_logits, _ = torch.topk(logits, top_k)\n",
    "            min_top_k = top_k_logits[:, -1].unsqueeze(-1)\n",
    "            logits = torch.where(logits < min_top_k, float('-inf'), logits)\n",
    "\n",
    "        # Apply top-p\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift the mask right to keep the first token above the threshold\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = False\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                dim=1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "            )\n",
    "            logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _detect_disallowed_sequence(self, generated_sequence: List[int]) -> Tuple[Tuple[int, ...], int]:\n",
    "        \"\"\"\n",
    "        Detects if the recent tokens in the generated_sequence match any disallowed sequence.\n",
    "\n",
    "        Args:\n",
    "            generated_sequence (List[int]): The list of generated token IDs.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[int, ...], int]: The matched disallowed sequence and its start position.\n",
    "                                         Returns (None, -1) if no match is found.\n",
    "        \"\"\"\n",
    "        # Start checking from the longest possible sequence to the shortest\n",
    "        for seq_length in range(self.max_sequence_length, 0, -1):\n",
    "            if len(generated_sequence) < seq_length:\n",
    "                continue\n",
    "            candidate_sequence = tuple(generated_sequence[-seq_length:])\n",
    "            if candidate_sequence in self.token_sequences:\n",
    "                start_pos = len(generated_sequence) - seq_length\n",
    "                return candidate_sequence, start_pos\n",
    "        return None, -1\n",
    "\n",
    "    def _display_debug(self, message: str):\n",
    "        \"\"\"\n",
    "        Displays debug information in the debug_output widget.\n",
    "\n",
    "        Args:\n",
    "            message (str): The debug message to display.\n",
    "        \"\"\"\n",
    "        with debug_output:\n",
    "            debug_output.clear_output(wait=True)\n",
    "            display(HTML(f\"<pre>{message}</pre>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Prompt</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa1052e4234426fb9e806033a44b68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Inference Output</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f20ffef8fc40819a63bed3021f1055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Debug Information</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8efe81c23a4c6d88280b42072e0e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup separate output widgets for prompt, inference, and debug\n",
    "prompt_output = Output()\n",
    "inference_output = Output()\n",
    "debug_output = Output()\n",
    "\n",
    "# Display the output widgets\n",
    "display(HTML(\"<h2>Prompt</h2>\"))\n",
    "display(prompt_output)\n",
    "display(HTML(\"<h2>Inference Output</h2>\"))\n",
    "display(inference_output)\n",
    "display(HTML(\"<h2>Debug Information</h2>\"))\n",
    "display(debug_output)\n",
    "\n",
    "# Enable slow debug mode\n",
    "SLOW_DEBUG = True\n",
    "\n",
    "# Initialize the sampler\n",
    "sampler = AdvancedCustomWordSampler(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    word_penalties=word_penalties,\n",
    "    starting_tokens_lookup=starting_tokens_lookup,\n",
    "    max_backtrack=5,\n",
    "    adjustment_strength=1.0,\n",
    "    device=device,\n",
    "    output_every_n_tokens=5,\n",
    "    slow_debug=SLOW_DEBUG,          # Enable slow debug\n",
    "    debug_delay=1.5                # Set delay to 1.5 seconds per debug step\n",
    ")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Write a story about Elara, the weaver of tapestries in future Technopolis. In the bustling city, a group of \"\n",
    "\n",
    "# Display the prompt\n",
    "with prompt_output:\n",
    "    prompt_output.clear_output(wait=True)\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap;'>{prompt}</div>\"))\n",
    "\n",
    "# Start generating\n",
    "try:\n",
    "    for text_chunk in sampler.generate_stream(prompt, max_length=300, temperature=1.0, top_k=50, top_p=0.95):\n",
    "        pass  # The text_chunk is already being displayed via the inference_output widget\n",
    "    print(\"\\nGeneration complete.\")\n",
    "except Exception as e:\n",
    "    with debug_output:\n",
    "        debug_output.clear_output(wait=True)\n",
    "        debug_output.append_stdout(f\"\\n\\nAn error occurred: {str(e)}\\n\")\n",
    "        traceback.print_exc(file=sys.stdout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
