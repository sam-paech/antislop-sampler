{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'antislop-sampler'...\n",
      "remote: Enumerating objects: 62, done.\u001b[K\n",
      "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
      "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
      "remote: Total 62 (delta 29), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (62/62), 2.40 MiB | 676.00 KiB/s, done.\n",
      "Resolving deltas: 100% (29/29), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sam-paech/antislop-sampler.git\n",
    "!mv antislop-sampler antislop_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from antislop_sampler.antislop_generate import generate_antislop, chat_antislop\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#model.pad_token_id = tokenizer.eos_token_id\n",
    "model.to(device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if os.path.exists('antislop_sampler/slop_phrase_prob_adjustments.json'):\n",
    "    with open('antislop_sampler/slop_phrase_prob_adjustments.json', 'r') as f:\n",
    "        slop_phrase_prob_adjustments = dict(json.load(f)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a story about Elara, the weaver of tapestries in future Technopolis. In the bustling city, a group of \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In the year 2178, in the sprawling city of New Elyria, the art of weaving tapestries had reached its peak. In a small, yet cozy shop tucked away in a quiet alley, a lone weaver named Elian worked day and night to create some of the most exquisite and sought-after pieces in the city. Her name was whispered among the locals as the Weaver of Dreams, a master weaver with a deep connection to the threads of fate and the fabric of reality.\n",
      "\n",
      "Elian's shop, \"Threads of Elyria,\" was a haven for those seeking tales of the past, present, and future. The walls were adorned with an array of woven tapestries, each one telling a different story, woven with care and precision by Elian's skilled hands. People from all corners of the city would come to visit her, hoping to unravel the mysteries hidden within the threads.\n",
      "\n",
      "Among them was a young apprentice named Lyra, who had heard tales of the Weaver of Dreams from her mother. Lyra was a curious and ambitious young woman, with a passion for storytelling and a thirst for knowledge. She had always dreamed of one day becoming a master weaver herself, but her father, a renowned historian, had discouraged her from pursuing a life of weaving, fearing it would be a mundane occupation.\n",
      "\n",
      "One day, Lyra's father fell ill, and as he lay on his deathbed, he handed her a small, intricately woven thread. It was a fragment of a long-forgotten tale, one that told of a hidden world beyond the city, where magic and technology merged in ways both wondrous and terrifying. The thread"
     ]
    }
   ],
   "source": [
    "# Chat generation with streaming\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "for token in chat_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=messages,\n",
    "    max_length=400,\n",
    "    # Antislop sampling may be less reliable at low temperatures.\n",
    "    temperature=1,    \n",
    "    min_p=0.1,\n",
    "    # The adjustment_strength param scales how strongly the probability adjustments are applied.\n",
    "    # A value of 1 means the values in slop_phrase_prob_adjustments (or the defaults) are used unmodified.\n",
    "    # Reasonable values are 0 (disabled) thru 10+ (effectively banning the list).\n",
    "    adjustment_strength=10.0,\n",
    "    # Optional: Provide a list of slop phrases and probability adjustments\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    streaming=True\n",
    "):\n",
    "    print(tokenizer.decode(token), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In the heart of Future Polis, where the sun dipped into the horizon and painted the sky with hues of crimson and gold, a young weaver named Elian stood at the edge of the city's central square. The air was alive with the hum of hoverbikes and the chatter of pedestrians, the smells of street food and machinery wafting through the air.\n",
      "\n",
      "For as long as anyone could remember, the people of Polis had relied on Elian's skills as a weaver to create the city's iconic tapestries. Her hands moved deftly, the threads of silver and gold dancing across her loom as she wove tales of the city's history and mythologies. Her fingers moved with the speed and precision of a surgeon, each stitch a tiny piece of Polis's rich cultural heritage.\n",
      "\n",
      "Elian's latest commission was a grand one – a massive mural depicting the founding of the city, the great hero, Arin the Unyielding, and his brave warriors. It would be a masterpiece, a celebration of Polis's triumph and the triumphs of its people.\n",
      "\n",
      "As she worked, Elian's mind wandered to the people of the city's lower districts, who struggled to make ends meet. The city was a melting pot of cultures, a place where the old and the new blended together in a swirling dance of innovation and tradition. But beneath the surface, tensions simmered – tensions between the haves and the have-nots, between those who had and those who had not.\n",
      "\n",
      "One night, as Elian worked late into the night, a young apprentice, Kael, crept into the square, his eyes scanning the\n"
     ]
    }
   ],
   "source": [
    "# Chat generation without streaming\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "generated_text = chat_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=messages,\n",
    "    max_length=400,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    adjustment_strength=2.0,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    streaming=False\n",
    ")\n",
    "print(tokenizer.decode(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urchins and street children stumble upon her loom, and they discover that her tapestries are not just beautiful works of art, but also hold a powerful secret.\n",
      "As the urchins explore the city's hidden corners, they notice that the tapestries seem to be changing, subtly shifting and rearranging themselves. At first, they think it's just a trick of the light, but as they watch, the tapestries begin to tell a story of their own – a tale of love, loss, and the struggles of growing up.\n",
      "\n",
      "The urchins, led by a curious and adventurous young girl named Aria, become obsessed with unraveling the mystery of the tapestries. They spend their days scouring the city for more clues, and their nights exploring the hidden alleys and courtyards, searching for any sign of the mysterious weaver.\n",
      "\n",
      "As they delve deeper into the city, they begin to notice that the tapestries are not just telling stories, but also revealing hidden truths about the city's inhabitants. They see glimpses of people's pasts, their fears, and their dreams. The tapestries seem to be a window into the city's soul, and the urchins are determined to learn more.\n",
      "\n",
      "One night, as they gather around a fire in a hidden courtyard, they discover a\n"
     ]
    }
   ],
   "source": [
    "# generate without streaming\n",
    "generated_text = generate_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=300,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    adjustment_strength=2.0,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    streaming=False\n",
    ")        \n",
    "print(tokenizer.decode(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12 of the most skilled artisans work together to create the grand tapestries that adorn the walls of the city's central square.\n",
      "\n",
      "The city of Newhaven is a marvel of innovation and progress, where technology and magic coexist in a symbiotic relationship. The air is alive with the hum of machines, and the streets are paved with a glittering substance called Glimmerstone, which is harvested from the crystals that line the city's skyscrapers. The Glimmerstone is used to power the city's infrastructure, and it's also a key component in the creation of the city's famous tapestries.\n",
      "\n",
      "In the heart of Newhaven's central square, a group of 12 artisans gather to work on their most ambitious project yet: the \"Elysian Mural\". This massive wall of tapestries will be the centerpiece of the square, and it will be the pride of the city. Each artisan has been selected for their unique skills and expertise, and they come from different walks of life.\n",
      "\n",
      "There's Arin, the master weaver from the ancient district of Luminaria, who has spent years perfecting his craft. He's known for his breathtaking patterns and colors, which seem to shift and change depending on the light. Next to him sits Lyra, the skilled weaver from the sprawling district of Nex"
     ]
    }
   ],
   "source": [
    "# generate with streaming\n",
    "for token in generate_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=300,\n",
    "    temperature=1,\n",
    "    min_p=0.1,\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    adjustment_strength=2.0,\n",
    "    streaming=True\n",
    "):\n",
    "    print(tokenizer.decode(token), end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
