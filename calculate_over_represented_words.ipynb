{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how to compute the over-represented words in a corpus of text.\n",
    "\n",
    "It works by counting word frequencies in the designated dataset (in this case a large set of LLM-generated creative writing). These frequencies are compared that against the wordfreq frequencies, which represent the average prevalence of words in (human) language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install wordfreq datasets numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from collections import Counter\n",
    "from wordfreq import word_frequency\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_datasets():\n",
    "    datasets_info = [\n",
    "        (\"ajibawa-2023/General-Stories-Collection\", \"train\")\n",
    "    ]\n",
    "    return {name: datasets.load_dataset(name, split=split) for name, split in datasets_info}\n",
    "\n",
    "def parse_text(datasets):\n",
    "    texts = []\n",
    "    for example in tqdm(datasets[\"ajibawa-2023/General-Stories-Collection\"]):\n",
    "        texts.append(example['text'])\n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_word_counts(texts, min_length=4):\n",
    "    \"\"\"\n",
    "    Count word frequencies in a list of texts.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (iterable of str): The input texts to process.\n",
    "    - min_length (int): Minimum length of words to include.\n",
    "\n",
    "    Returns:\n",
    "    - Counter: A Counter object mapping words to their frequencies.\n",
    "    \"\"\"\n",
    "    # Precompile the regex pattern for better performance\n",
    "    # This pattern matches words with internal apostrophes (e.g., \"couldn't\")\n",
    "    pattern = re.compile(r\"\\b\\w+(?:'\\w+)?\\b\")\n",
    "    \n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"Counting words\"):\n",
    "        if not isinstance(text, str):\n",
    "            continue  # Skip non-string entries to make the function more robust\n",
    "        \n",
    "        # Convert to lowercase and find all matching words\n",
    "        words = pattern.findall(text.lower())\n",
    "        \n",
    "        # Update counts with words that meet the minimum length\n",
    "        word_counts.update(word for word in words if len(word) >= min_length)\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def analyze_word_rarity(word_counts):\n",
    "    total_words = sum(word_counts.values())\n",
    "    corpus_frequencies = {word: count / total_words for word, count in word_counts.items()}\n",
    "    \n",
    "    wordfreq_frequencies = {word: word_frequency(word, 'en') for word in word_counts.keys()}\n",
    "    \n",
    "    # Filter out words with zero frequency\n",
    "    valid_words = [word for word, freq in wordfreq_frequencies.items() if freq > 0]\n",
    "    \n",
    "    corpus_freq_list = [corpus_frequencies[word] for word in valid_words]\n",
    "    wordfreq_freq_list = [wordfreq_frequencies[word] for word in valid_words]\n",
    "    \n",
    "    # Calculate average rarity\n",
    "    avg_corpus_rarity = np.mean([-np.log10(freq) for freq in corpus_freq_list])\n",
    "    avg_wordfreq_rarity = np.mean([-np.log10(freq) for freq in wordfreq_freq_list])\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(corpus_freq_list, wordfreq_freq_list)[0, 1]\n",
    "    \n",
    "    return corpus_frequencies, wordfreq_frequencies, avg_corpus_rarity, avg_wordfreq_rarity, correlation\n",
    "\n",
    "def find_over_represented_words(corpus_frequencies, wordfreq_frequencies, top_n=50000):\n",
    "    over_representation = {}\n",
    "    for word in corpus_frequencies.keys():\n",
    "        wordfreq_freq = wordfreq_frequencies[word]\n",
    "        if wordfreq_freq > 0:  # Only consider words with non-zero frequency\n",
    "            over_representation[word] = corpus_frequencies[word] / wordfreq_freq\n",
    "    \n",
    "    return sorted(over_representation.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "def find_zero_frequency_words(word_counts, wordfreq_frequencies, top_n=50000):\n",
    "    zero_freq_words = {word: count for word, count in word_counts.items() if wordfreq_frequencies[word] == 0}\n",
    "    return sorted(zero_freq_words.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Downloading datasets...\")\n",
    "all_datasets = download_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Parsing text...\")\n",
    "texts = parse_text(all_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Total texts extracted: {len(texts)}\")\n",
    "\n",
    "print(\"Counting words...\")\n",
    "word_counts = get_word_counts(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_mostly_numeric(word_counts):\n",
    "    def is_mostly_numbers(word):\n",
    "        digit_count = sum(c.isdigit() for c in word)\n",
    "        return digit_count / len(word) > 0.2  # Adjust this ratio if needed\n",
    "    \n",
    "    # Create a new Counter with filtered words\n",
    "    return Counter({word: count for word, count in word_counts.items() if not is_mostly_numbers(word)})\n",
    "\n",
    "filtered_counts = filter_mostly_numeric(word_counts)\n",
    "\n",
    "print(\"Analyzing word rarity...\")\n",
    "corpus_frequencies, wordfreq_frequencies, avg_corpus_rarity, avg_wordfreq_rarity, correlation = analyze_word_rarity(filtered_counts)\n",
    "\n",
    "print(f\"Total unique words analyzed: {len(word_counts)}\")\n",
    "print(f\"Average corpus rarity: {avg_corpus_rarity:.4f}\")\n",
    "print(f\"Average wordfreq rarity: {avg_wordfreq_rarity:.4f}\")\n",
    "print(f\"Correlation between corpus and wordfreq frequencies: {correlation:.4f}\")\n",
    "\n",
    "print(\"\\nMost over-represented words in the corpus:\")\n",
    "over_represented = find_over_represented_words(corpus_frequencies, wordfreq_frequencies)\n",
    "for word, score in over_represented:\n",
    "    print(f\"{word}: {score:.2f} times more frequent than expected\")\n",
    "\n",
    "print(\"\\nMost frequent words with zero wordfreq frequency:\")\n",
    "zero_freq_words = find_zero_frequency_words(filtered_counts, wordfreq_frequencies)\n",
    "for word, count in zero_freq_words:\n",
    "    print(f\"{word}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "with open('over_represented_words.json', 'w') as f:\n",
    "    json.dump(over_represented, f)\n",
    "with open('frequent_non_dictionary_words.json', 'w') as f:\n",
    "    json.dump(zero_freq_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_frequencies['testament']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
